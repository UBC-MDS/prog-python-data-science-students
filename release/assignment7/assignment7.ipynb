{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning  \n",
    "\n",
    "## Assignment 7: Assessment and Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't learn technical subjects without hands-on practice. The assignments are an important part of the course. To submit this assignment you will need to make sure that you save your Jupyter notebook. \n",
    "\n",
    "Below are the links of 2 videos that explain:\n",
    "\n",
    "1. [How to save your Jupyter notebook](https://youtu.be/0aoLgBoAUSA) and,       \n",
    "2. [How to answer a question in a Jupyter notebook assignment](https://youtu.be/7j0WKhI3W4s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Learning Goals:\n",
    "\n",
    "By the end of the module, students are expected to:\n",
    "\n",
    "- Explain why accuracy is not always the best metric in ML.\n",
    "- Explain components of a confusion matrix.\n",
    "- Define precision, recall, and f1-score and use them to evaluate different classifiers.\n",
    "- Identify whether there is class imbalance and whether you need to deal with it.\n",
    "- Explain `class_weight` and use it to deal with data imbalance.\n",
    "- Appropriately select a scoring metric given a regression problem.\n",
    "- Interpret and communicate the meanings of different scoring metrics on regression problems. MSE, RMSE, $R^2$, MAPE.\n",
    "- Apply different scoring functions with `cross_validate` and `GridSearchCV` and `RandomizedSearchCV`.\n",
    "\n",
    "\n",
    "This assignment covers [Module 7](https://ml-learn.mds.ubc.ca/en/module7) of the online course. You should complete this module before attempting this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any place you see `...`, you must fill in the function, variable, or data to complete the code. Substitute the `None` with your completed code and answers then proceed to run the cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the questions in this assignment will have hidden tests. This means that no feedback will be given as to the correctness of your solution. It will be left up to you to decide if your answer is sufficiently correct. These questions are worth 2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for this lab\n",
    "from hashlib import sha1\n",
    "\n",
    "import altair as alt\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import make_column_transformer \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    Normalizer,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    normalize,\n",
    "    scale)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "import test_assignment7 as t\n",
    "#alt.renderers.enable('mimetype')\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Precision, recall, and f1 score \"by hand\" (without `sklearn`)\n",
    "\n",
    "\n",
    "Consider the problem of predicting whether a patient has a disease or not. Below are confusion matrices of two machine learning models: Model A and Model B. \n",
    "\n",
    "##### Model A\n",
    "|    Actual/Predicted      | Predicted disease | Predicted no disease |\n",
    "| :------------- | -----------------------: | -----------------------: |\n",
    "| **Actual disease**       | 2 | 8 |\n",
    "| **Actual no disease**       | 0 | 100 |\n",
    "\n",
    "\n",
    "##### Model B\n",
    "|    Actual/Predicted      | Predicted disease | Predicted no disease |\n",
    "| :------------- | -----------------------: | -----------------------: |\n",
    "| **Actual disease**       | 6 | 4 |\n",
    "| **Actual no disease**       | 10 | 90 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** <br> {points: 1}  \n",
    "\n",
    "Precision, recall, and f1 score depend crucially upon which class is considered \"positive\", that is the thing you wish to find. In the example above, which class is likely to be the \"positive\" class?\n",
    "\n",
    "Save the label name in a string object named `answer_1_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9eb12811100c161cf3c213fe35709f21",
     "grade": false,
     "grade_id": "cell-10bccd38560266fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_1_1 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adbcee21c3dfdda7bdf609a0bba2836d",
     "grade": true,
     "grade_id": "cell-58c8cbee8ef77632",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_1(answer_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** <br> {points: 3}  \n",
    "\n",
    "Calculate accuracies for Model A and Model B. \n",
    "\n",
    "Save the values of each calculations as a fraction in objects name `model_a_acc` and `model_b_acc` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f758987de44065f042166873d2d03e4a",
     "grade": false,
     "grade_id": "cell-da59f7cfdbd13642",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_a_acc = None\n",
    "model_b_acc = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5da7b795c782fad3ecd1b4a46e86a3f1",
     "grade": true,
     "grade_id": "cell-112a1f1642f468fa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_2_1(model_a_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5439fe795a81ef68b4669b3f249316dd",
     "grade": true,
     "grade_id": "cell-7adaec399eb35deb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'model_b_acc' in globals(\n",
    "), \"Please make sure that your solution is named 'model_b_acc'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3** <br> {points: 1}  \n",
    "\n",
    "Which model would you pick simply based on the accuracy metric? \n",
    "\n",
    "Save either \"Model A\" or \"Model B\" in an object named `answer_1_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9df5766cefde9a9e9fa02bb2c9228a74",
     "grade": false,
     "grade_id": "cell-88e8a579efb8b86f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_1_3 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "695f0efead049219fb2dcfc351bdb71a",
     "grade": true,
     "grade_id": "cell-6d1051aef2f7edba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_3(answer_1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4** <br> {points: 3}  \n",
    "\n",
    "Calculate precision, recall, f1-score for **Model A** by designating the appropriate fraction to objects named `a_precision`, `a_recall` and `a_f1`. \n",
    "\n",
    "You can use the objects `a_precision` and `a_recall` to use in your `a_f1` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49c3b792195c18ca1862fac873a0ebbd",
     "grade": false,
     "grade_id": "cell-4f35b1710839f096",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a_precision = None\n",
    "a_recall = None\n",
    "a_f1 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb4920f56cf39a031adf747b2edb144d",
     "grade": true,
     "grade_id": "cell-ebc2c04c38d967f8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4_1(a_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4a6fd12459d024e700bac427d0588bd",
     "grade": true,
     "grade_id": "cell-e6873091826ecbfe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4_2(a_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf10040bc55dcb1d892339cacec1f878",
     "grade": true,
     "grade_id": "cell-22acb42332686294",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_4_3(a_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5** <br> {points: 3}  \n",
    "\n",
    "Calculate precision, recall, f1-score for **Model B** by designating the appropriate fraction to objects named `b_precision`, `b_recall` and `b_f1`. \n",
    "\n",
    "You can use the objects `b_precision` and `b_recall` to use in your `b_f1` calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86355ab435db5480187ba8edc86054b0",
     "grade": false,
     "grade_id": "cell-1464d82d29d554be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "b_precision = None\n",
    "b_recall = None\n",
    "b_f1 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a8846c7780c15cd0ba596718f2c0ce1",
     "grade": true,
     "grade_id": "cell-c41145a09c7bfc8b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_1(b_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20a1bbdd0ae50247b9003023be15fa83",
     "grade": true,
     "grade_id": "cell-6564d4b3d12c0f51",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_2(b_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61278e395335b95ca3af119b70169b1b",
     "grade": true,
     "grade_id": "cell-a97d0ee2aa39b97c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_5_3(b_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6** <br> {points: 1}  \n",
    "\n",
    "Which metric is more informative in this case?\n",
    "\n",
    "i) Accuracy\n",
    "\n",
    "ii) Precision\n",
    "\n",
    "iii) Recall\n",
    "\n",
    "iv) f1\n",
    "\n",
    "\n",
    "\n",
    "Select all that apply and add them into a list named `answer_1_6`. \n",
    "For example if statement i and iv are both true, your solution will look like this: \n",
    "\n",
    "```\n",
    "answer_1_6 = [\"i\", \"iv\"] \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb0b62c5b0974c0126f91063234f13bc",
     "grade": false,
     "grade_id": "cell-94a5d47d39c26a6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_1_6 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02f02a17203cdb6875af1c4ba573b101",
     "grade": true,
     "grade_id": "cell-40689573c4b45ba7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_6(answer_1_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.7** <br> {points: 1}  \n",
    "\n",
    "Which model would you pick based on this information? \n",
    "\n",
    "Save either \"Model A\" or \"Model B\" in an object named `answer_1_7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0e1d55a4feedcd1b09a39cbc1b56428",
     "grade": false,
     "grade_id": "cell-4dd94c25673682b2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_1_7 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c67be0c0ff2a40c752c9078d6ee1ac1d",
     "grade": true,
     "grade_id": "cell-3b1fd3696243d2e6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_1_7(answer_1_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification evaluation metrics using `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when a dataset is imbalanced, accuracy does not provide the whole story. In the module material, we looked at a credit card fraud dataset which is a classic example of an imbalanced dataset. Another example is customer churn datasets. For the next questions, you will be using a [bank customer churn dataset](https://www.kaggle.com/shubh0799/churn-modelling) from Kaggle. In this question, we will be concentrating on the target label `Exited`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv(\"data/churn.csv\")\n",
    "train_df, test_df = train_test_split(churn_df, test_size=0.3, random_state=123)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1** <br> {points: 1}  \n",
    "\n",
    "What is the distribution of target values (`Exited`) in the train split? Your answer should be of type Pandas Series and saved in an object named `class_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44665c095202fa383686378d2c4c1883",
     "grade": false,
     "grade_id": "cell-8fee174efb95046b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class_dist = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa37bbafe6269aa3afd61bfa4b599abb",
     "grade": true,
     "grade_id": "cell-9693a3487c29667f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_1(class_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** <br> {points: 1}  \n",
    "\n",
    "Let's now separate our feature vectors from the target.\n",
    "\n",
    "Use all the columns except for `Exited` as your `X` and the `Exited` column as your target `y`. \n",
    "\n",
    "You will need to split both `train_df` and `test_df`. \n",
    "\n",
    "Save the results in objects named `X_train`, `y_train`, `X_test` and `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7222433fd61a1eb03f0ef20132bb45f",
     "grade": false,
     "grade_id": "cell-912921e7e74800fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = None, None, None, None\n",
    "    \n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6568c622b7399949d5a1464f03764be0",
     "grade": true,
     "grade_id": "cell-f7c38b0ae6f70af2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_2(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** <br> {points: 1} \n",
    "\n",
    "Carry out cross-validation with `DummyClassifier` using the `stratified` strategy. Pass the following `scoring` metrics to `cross_validate`. \n",
    "- accuracy\n",
    "- f1\n",
    "- recall\n",
    "- precision\n",
    "\n",
    "Make sure you use  `return_train_score=True` and 5-fold cross-validation.\n",
    "\n",
    "Save your results in a dataframe named `dummy_scores`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56ea1d42d35997c8899400fa353161b6",
     "grade": false,
     "grade_id": "cell-c6682256ddbc4a46",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_scores = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33b2620699332d7a72bfbbf4d5090a9c",
     "grade": true,
     "grade_id": "cell-f74782b5290c5565",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_3(dummy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** <br> {points: 1} \n",
    "\n",
    "What is the mean of each column in `dummy_scores`?\n",
    "\n",
    "\n",
    "Save your result in an object named `dummy_mean`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d74ea25a828ef05c7e99569d8315da4",
     "grade": false,
     "grade_id": "cell-99de59fe13557284",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_mean = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad18f1577384e05167832bde2092cf42",
     "grade": true,
     "grade_id": "cell-57af51df99cb0bdf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_4(dummy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5** <br> {points: 5}  \n",
    "\n",
    "Using either `.describe(include=\"all\")`, `.info()` or looking at the data, split it into 4 types of features; numeric, categorical, binary and `drop_features`. \n",
    "\n",
    "- Add the labels of the numeric column(s) (as type string) to a list name `numeric_features`.\n",
    "- Add the labels of the categorical column(s) (as type string) to a list name `categorical_features`.\n",
    "- Add the labels of the binary column(s) (as type string) to a list name `binary_features`.\n",
    "- Add the labels of the column that should be excluded from the model fitting to a list named `drop_features`. _(Hint: which of the columns is a unique identifier for the examples?)_\n",
    "\n",
    "Identify different feature types (e.g., numeric, categorical, binary, drop features).\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "276c576ccc6e6b210cd4ba1ae0cc7360",
     "grade": false,
     "grade_id": "cell-66dbf242bf1a9faa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = None\n",
    "categorical_features = None\n",
    "drop_features = None\n",
    "binary_features = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e01d54db20b8c570ddc3c2e4bf8c4ef",
     "grade": true,
     "grade_id": "cell-b422078fdd00a9a8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_5_1(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6593b949763776cecf0d519875726fbb",
     "grade": true,
     "grade_id": "cell-fd94cbdf40c54387",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'categorical_features' in globals(\n",
    "), \"Please make sure that your solution is named 'categorical_features'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b319e1c5c2143f1c1facad9f2661aa27",
     "grade": true,
     "grade_id": "cell-9260945a70a8df44",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_5_3(drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba41741e8cd540f315c47d432d8e1294",
     "grade": true,
     "grade_id": "cell-8a7d80a0f3f69e0b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_5_4(binary_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have no null values we can skip the individual pipelines and make our column transforming with just a single transformation. \n",
    "\n",
    "Because everyone needs a little help once in a while, we are going to do this part for you! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    (OneHotEncoder(handle_unknown=\"error\", drop=\"if_binary\"), binary_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6** <br> {points: 2}  \n",
    "\n",
    "In this question, you will be using the [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) which you haven't studied yet but did see in assignment 3.  You should feel comfortable using models with our usual ML workflow even if you haven't seen them before. \n",
    "\n",
    "\n",
    "Build a pipeline named `unbalanced_pipe` that first preprocesses `preprocessor` and then builds a  `RandomForestClassifier` using a random_state of 77. \n",
    "\n",
    "Carry out cross-validation on `unbalanced_pipe` and the training set using the `cross_validate` function and the following evaluation metrics:\n",
    "- `accuracy`\n",
    "- `precision`\n",
    "- `recall`\n",
    "- `f1`\n",
    "\n",
    "Note that you can pass multiple [scoring metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) as a list or a dict to the `scoring` parameter. \n",
    "\n",
    "Save the results of the cross-validation in a dataframe named`rf_unbalanced_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9e544e4a429e0c47241b64eb1eb8d2e",
     "grade": false,
     "grade_id": "cell-0de662581b33ae12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "unbalanced_pipe = None\n",
    "rf_unbalanced_scores = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf19a4f79b7fb0b22960cae2227b3a22",
     "grade": true,
     "grade_id": "cell-e578cb24650f0946",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_6(rf_unbalanced_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7** <br> {points: 1} \n",
    "\n",
    "What is the mean of each column in `rf_unbalanced_scores`?\n",
    "\n",
    "\n",
    "Save your result in an object named `rf_unbalanced_mean`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3689cb896183d89fb73b66b95b686e08",
     "grade": false,
     "grade_id": "cell-22fba680c83299d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rf_unbalanced_mean = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "686881db3c233c199a755d522cff5785",
     "grade": true,
     "grade_id": "cell-472ac701d5761924",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_7(rf_unbalanced_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8** <br> {points: 2}  \n",
    "\n",
    "Repeat question 6 above but this time set `class_weight=\"balanced\"` in the `RandomForestClassifier`. \n",
    "Save the new pipeline in an object named `balanced_pipe`. Don't forget to use `random_state=77`for the classifier. \n",
    "\n",
    "Carry out cross-validation on `balanced_pipe` but this time save the scores in a dataframe named `rf_balanced_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "202526d1904a72b94b36106669929ff9",
     "grade": false,
     "grade_id": "cell-1c4687b4a9a6aac8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "balanced_pipe = None\n",
    "rf_balanced_scores = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a7620a4c80f3634f445ffa7ae772dcd",
     "grade": true,
     "grade_id": "cell-49f5395fd9aa9edd",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_8(rf_balanced_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9** <br> {points: 1} \n",
    "\n",
    "What is the mean of each column in `rf_balanced_scores`?\n",
    "\n",
    "\n",
    "Save your result in an object named `rf_balanced_mean`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fd2ed2b7e126bb421670acc51b7e679",
     "grade": false,
     "grade_id": "cell-1d3c898d4c72428c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rf_balanced_mean = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3cf89f57851f482b3300f32b64441b5",
     "grade": true,
     "grade_id": "cell-f57fbaf001a3bd23",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_9(rf_balanced_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.10** <br> {points: 1} \n",
    "\n",
    "Look at your results above. Which of the following statements are true? \n",
    "\n",
    "i) Both random forest models have better mean validation accuracy performance than the dummy classifier.\n",
    "\n",
    "ii) The balanced class random forest model has a better mean validation recall score than the unbalanced random forest model. \n",
    "\n",
    "iii) The balanced class random forest model has a better mean validation precision score than the unbalanced random forest model.  \n",
    "\n",
    "iv) Mean validation precision and recall scores only increased in the balanced class random forest model.\n",
    "\n",
    "v) Both random forest models show higher validation precision scores than validation recall scores. \n",
    "\n",
    "\n",
    "Select all that apply and add them into a list named `answer_2_10`. \n",
    "For example if statement i and iv are both true, your solution will look like this: \n",
    "\n",
    "```\n",
    "answer_2_10 = [\"i\", \"iv\"] \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f28fc9c7029911084e74b483324d565",
     "grade": false,
     "grade_id": "cell-6de147c823c76215",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_2_10 = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9277f4f016fc9760497d21a42eee8b91",
     "grade": true,
     "grade_id": "cell-fd3c4b417ac834cf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_10(answer_2_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.11** <br> {points: 1} \n",
    "\n",
    "For this next question, we have given you the majority of the code to hyperparameter tune the balanced `RandomForestClassifier`. \n",
    "\n",
    "Copy and paste the contents of the cell below into the following cell where the solution should be. Fill in the `....` blanks below so that it executes and returns the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffd24bba30c0a1d3a58e25b2f66331b1",
     "grade": false,
     "grade_id": "cell-02e169611793d840",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import randint\n",
    "\n",
    "rf_pipeline = make_pipeline(\n",
    "    preprocessor, ....(class_weight=...., random_state=123)\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"randomforestclassifier__n_estimators\": scipy.stats.randint(low=10, high=300),\n",
    "    \"randomforestclassifier__max_depth\": scipy.stats.randint(low=2, high=20),\n",
    "}\n",
    "\n",
    "random_search = ....(\n",
    "    ....,\n",
    "    ....,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    ....=\"f1\",\n",
    "    random_state=123,\n",
    ")\n",
    "random_search.....(X_train, y_train)\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fcb352063b684bc4bbf4808ab24923c",
     "grade": true,
     "grade_id": "cell-329ba0ee56f73721",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_11(rf_pipeline, random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.12** <br> {points: 3}\n",
    "\n",
    "What are the best hyperparameter value for `n_estimators` and `max_depth`. Save it in an object named `optimal_parameters` (The auto-grader is expecting a dictionary object). \n",
    "\n",
    "What was the corresponding validation score? Save this in an object named `optimal_score`. \n",
    "\n",
    "*Hint: `.best_params_`  and `.best_score_` are helpful here.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc6c6575c2fc2f9b1689150b0ec3740a",
     "grade": false,
     "grade_id": "cell-e5054e9f65a2ecdc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_parameters = None\n",
    "optimal_score = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d4f2e85d105299fdd9f45122c039b41",
     "grade": true,
     "grade_id": "cell-f6d885ec6d0c2d90",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t. test_2_12_1(random_search, optimal_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12d70590bfd258cf23c9fc2abd48421f",
     "grade": true,
     "grade_id": "cell-1f755ca4548b90ca",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'optimal_score' in globals(\n",
    "), \"Please make sure that your solution is named 'optimal_score'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.13** <br> {points: 2} \n",
    "\n",
    "What is the train and test score of the best scoring model? Save the result in objects named `training_score`, and `testing_score` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd39d4e92a20e66646b0ddc210d40c63",
     "grade": false,
     "grade_id": "cell-f0ab8c39cbe9ed7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "training_score = None\n",
    "testing_score = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af7aabfd55244b516249b89f198be417",
     "grade": true,
     "grade_id": "cell-b8e8776e62548a4e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_13_1(training_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2657a56f9634f58c9be5cad73a40b688",
     "grade": true,
     "grade_id": "cell-61c66a15b802b184",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_13_2(testing_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.14** <br> {points: 1} \n",
    "\n",
    "Import the appropriate libraries to plot a confusion matrix and print a classification report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "509809b5c20010d132e2e040d1da4301",
     "grade": false,
     "grade_id": "cell-1329e2207ccdb878",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5da55e96cc2561e825a349f3b4f0fafb",
     "grade": true,
     "grade_id": "cell-7cef3fe5c8c08343",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_14()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.15** <br> {points: 1} \n",
    "\n",
    "\n",
    "Plot a confusion matrix on the test set using the object `random_search` as your estimator and \"normalize\" all your results (see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html) for more help here).\n",
    "\n",
    "Name the plot `cm_plot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "480e44315ece9604fa6621ffc40c4328",
     "grade": false,
     "grade_id": "cell-d4ee02e8d0ad2f1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cm_plot = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e85bf1634b99ccb557f223da6ba10cf",
     "grade": true,
     "grade_id": "cell-e926b5a0a1f975c5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_15(cm_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.16** <br> {points: 3} \n",
    "\n",
    "Below print a classification report on the `X_test` predictions of `random_search`'s best model. Use this information to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to print your classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A) What is the recall considering that `Exited`(=1) is our \"positive\" class? Save the result to two decimal places in an object named `answer2_16a`. \n",
    "\n",
    "B) What is the precision weighted average? Save the result to two decimal places in an object named `answer2_16b`. \n",
    "\n",
    "C) How many customers exited in this test set? Save the result as an integer in an object named `answer2_16c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd6111addf2a810a4a507c9a53c947f3",
     "grade": false,
     "grade_id": "cell-2a0327eb7dbf23a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer2_16a = None\n",
    "answer2_16b = None\n",
    "answer2_16c = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302ef6c3fc4b82d6f5181781ce1b5da8",
     "grade": true,
     "grade_id": "cell-b4b39de2552f60bc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_16_1(answer2_16a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b52becccfbd8d1295c376954dbeb3928",
     "grade": true,
     "grade_id": "cell-c4325891305a9cae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_16_2(answer2_16b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8062b3be0407e75ee0d135a3599d875",
     "grade": true,
     "grade_id": "cell-13dafcb469b21ef5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_16_3(answer2_16c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.17** <br> {points: 1} \n",
    "\n",
    "What's happened to our model since we have tuned our hyperparameters?\n",
    "\n",
    "A) We have sacrificed some of our precision score for a better recall score.\n",
    "\n",
    "B) Our accuracy has decreased.\n",
    "\n",
    "C) We have likely increased the number of falsely identified exited customers.\n",
    "\n",
    "D) All of the above.\n",
    "\n",
    "*Answer in the cell below using the uppercase letter associated with your answer. Place your answer between `\"\"`, assign the correct answer to an object called `answer2_17`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ace948d39a7b28eb7e874780ca05db55",
     "grade": false,
     "grade_id": "cell-edf2e1be7eceed82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer2_17 = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n",
    "answer2_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e75cf6d8e572e548aa27b98b0a251896",
     "grade": true,
     "grade_id": "cell-f0db9a56f75d6b9a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_2_17(answer2_17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will bring back the [Taiwan real estate valuation dataset](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set) that we saw in assignment 1. \n",
    "\n",
    "We are doing the first part for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv('data/real_estate.csv')\n",
    "train_df, test_df = train_test_split(housing_df, test_size=0.2, random_state=123)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['price'])\n",
    "y_train = train_df['price']\n",
    "\n",
    "X_test = test_df.drop(columns=['price'])\n",
    "y_test = test_df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1** <br> {points: 1}  \n",
    "\n",
    "Since we have seen this dataset already, we know there are only numeric features and no missing values so we can skip quite a few steps here. \n",
    "\n",
    "Make a single pipeline using `make_pipeline` named `reg_pipe` that first uses `StandardScaler()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9af4530f8eb18b97eb08ec40d3a2bf9",
     "grade": false,
     "grade_id": "cell-69d4e91a504c1837",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "reg_pipe = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4612d4f03ef95081fd478e668f33a43c",
     "grade": true,
     "grade_id": "cell-d8d5d76226e3a145",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_1(reg_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2** <br> {points: 1}  \n",
    "\n",
    "Given the MAPE function we have given below, create a scorer function that we can pass into cross-validation. (look [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html?highlight=make_scorer#sklearn.metrics.make_scorer) for help). \n",
    "\n",
    "Save it in an object named `mape_scorer`. Remember that MAPE is an error measure so we must use `greater_is_better=False` as an argument. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fde6497e6e3e0a29d03f941ef910781f",
     "grade": false,
     "grade_id": "cell-bfca9f71be42499d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mape(true, pred):\n",
    "    return 100.0 * np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f81e0134de01413a96b383f142c636e8",
     "grade": true,
     "grade_id": "cell-8a3cf905241ca8b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_2(mape_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3** <br> {points: 1}  \n",
    "\n",
    "Make a dictionary named `scoring_dict` consisting the following metrics: \n",
    "\n",
    "- `neg_mean_squared_error`\n",
    "- `neg_root_mean_square`\n",
    "- `neg_mean_absolute_error`\n",
    "- `r2`\n",
    "- `mape_scorer`\n",
    "\n",
    "The key-value pairs in this dictionary should be identical. (Aka the item to the left of the colon is the same as the right side of the colon - we have started it for you just in case.)\n",
    "\n",
    "Remember that `mape_scorer` is a variable and should **NOT** be called as a string. (Although we need to name it as a string value for the dictionary's key; See slide 6 of exercise 21 for help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c3641b48327a66e14164e8ad1a67466",
     "grade": false,
     "grade_id": "cell-b824fb2b08c194e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scoring_dict = {\"neg_mean_squared_error\": \"neg_mean_squared_error\",\n",
    "               }\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01f9e8795770ec95137b9ccb85f8cd6c",
     "grade": true,
     "grade_id": "cell-2262d2645f47df97",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_3(scoring_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** <br> {points: 1}  \n",
    "\n",
    "Carry out cross-validation on `reg_pipe` and the training set using the `cross_validate` function.\n",
    "\n",
    "Make sure to set `return_train_score=True` and assign our `scoring_dict` to the `scoring` argument.\n",
    "\n",
    "\n",
    "Save the results of the cross-validation in a dataframe named `regression_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2527d12fdfdf960f1d3acab42c5ddb5f",
     "grade": false,
     "grade_id": "cell-457e7772f822ea07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "regression_scores = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd455c6298edcf13a392ef3405e5012a",
     "grade": true,
     "grade_id": "cell-db06848393467bff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_4(regression_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5** <br> {points: 1} \n",
    "\n",
    "What is the mean of each column in `regression_scores`?\n",
    "\n",
    "\n",
    "Save your result in an object named `regression_mean`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d3a064443139b53c79cf608f0aab5f0",
     "grade": false,
     "grade_id": "cell-cfb55ddae37d8d25",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "regression_mean = None \n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25375c0d00b9b4c14317da96188d4586",
     "grade": true,
     "grade_id": "cell-4993cba10fcfe8c5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "t.test_3_5(regression_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6** <br> {points: 2} \n",
    "\n",
    "What is the model's mean validation MAPE score? \n",
    "\n",
    "Save your answer to 2 decimal places in an object named `mean_reg_mape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c594f14a6dc459dbbc2fd63f43fb5970",
     "grade": false,
     "grade_id": "cell-9cfd9da29de1973e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mean_reg_mape = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29975a2ec1f868886a0f90d1f13875f9",
     "grade": true,
     "grade_id": "cell-763f0a302ba20680",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the variable exists\n",
    "assert 'mean_reg_mape' in globals(\n",
    "), \"Please make sure that your solution is named 'mean_reg_mape'\"\n",
    "\n",
    "# This test has been intentionally hidden. It will be up to you to decide if your solution\n",
    "# is sufficiently good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributions\n",
    "- The Bank Customer Churn DataSet - [Kaggle](https://www.kaggle.com/shubh0799/churn-modelling)\n",
    "\n",
    "- Real Estate Dataset - [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set)\n",
    "\n",
    "*Yeh, I. C., & Hsu, T. K. (2018). Building real estate valuation models with comparative approach through case-based reasoning. Applied Soft Computing, 65, 260-271.*\n",
    "\n",
    "\n",
    "- MDS DSCI 573 - Feature & Model Selection  - [MDS's GitHub website](https://github.com/UBC-MDS/DSCI_573_feat-model-select) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Submitting \n",
    "\n",
    "Before submitting your assignment please do the following:\n",
    "\n",
    "- Read through your solutions\n",
    "- **Restart your kernel and clear output and rerun your cells from top to bottom** \n",
    "- Makes sure that none of your code is broken \n",
    "- Verify that the tests from the questions you answered have obtained the output \"Success\"\n",
    "\n",
    "This is a simple way to make sure that you are submitting all the variables needed to mark the assignment. This method should help avoid losing marks due to changes in your environment.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
